{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### DECISION TREE\n",
        "\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "ANSWER=> A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It works by splitting the dataset into smaller subsets based on feature values, forming a tree-like structure of decisions.\n",
        "\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer=> Gini Impurity measures how often a randomly chosen element would be incorrectly classified. It ranges from 0 (pure node) to 0.5 (for binary classification). A lower Gini value indicates a better split.Entropy measures the level of randomness or disorder in a dataset. Higher entropy means more uncertainty. Information Gain is calculated using entropy.\n",
        "Both metrics guide the tree to create splits that result in child nodes with higher purity. The split that minimizes impurity (Gini) or maximizes Information Gain (Entropy) is selected.\n",
        "\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each?\n",
        "\n",
        "Answer=>Pre-Pruning stops the tree from growing during training by applying constraints such as maximum depth or minimum samples per split.\n",
        "\n",
        "Advantage: Reduces overfitting and training time.\n",
        "\n",
        "Post-Pruning allows the tree to grow fully and then removes unnecessary branches after training.\n",
        "\n",
        "Advantage: Often achieves better generalization by simplifying the tree based on validation data.\n",
        "\n",
        "\n",
        "\n",
        " QUESTION 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "\n",
        "Answer=> Information Gain measures the reduction in entropy after splitting a dataset on a particular feature. It tells us how much information a feature provides about the target class.\n",
        "It is important because the feature with the highest Information Gain produces the most homogeneous child nodes, leading to better classification accuracy.\n",
        "\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Answer=>\n",
        "Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "Credit risk analysis\n",
        "Fraud detection\n",
        "Customer churn prediction\n",
        "Recommendation systems\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to interpret and visualize\n",
        "Works with numerical and categorical data\n",
        "Requires little data preprocessing\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting\n",
        "Sensitive to small data changes\n",
        "Can create biased trees if data is imbalanced\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7EG9KU3U2IG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOQxwp0-2Anm"
      },
      "outputs": [],
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "# Answer=>\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "#  Answer\n",
        "\n",
        "shallow_tree = DecisionTreeClassifier(max_depth=3)\n",
        "shallow_tree.fit(X_train, y_train)\n",
        "shallow_acc = accuracy_score(y_test, shallow_tree.predict(X_test))\n",
        "\n",
        "full_tree = DecisionTreeClassifier()\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "\n",
        "print(\"Shallow Tree Accuracy:\", shallow_acc)\n",
        "print(\"Full Tree Accuracy:\", full_acc)"
      ],
      "metadata": {
        "id": "06sLpmgS4Pg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Answer=>\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "id": "8B5Vhz0k4kRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "\n",
        "# Answer=>\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "'max_depth': [2, 3, 4, 5, None],\n",
        "'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "W84jGGyO40gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting\n",
        "\n",
        "Answer=>\n",
        "Handle Missing Values:\n",
        "\n",
        "Numerical: mean/median imputation\n",
        "\n",
        "Categorical: most frequent or separate category\n",
        "\n",
        "Encode Categorical Features:\n",
        "\n",
        "One-Hot Encoding or Label Encoding\n",
        "\n",
        "Train Decision Tree Model:\n",
        "\n",
        "Split data into training and testing sets\n",
        "\n",
        "Train using appropriate criterion\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Use GridSearchCV to tune max_depth, min_samples_split, etc.\n",
        "\n",
        "Evaluate Performance:\n",
        "\n",
        "Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC\n",
        "\n",
        "Business Value:\n",
        "\n",
        "Early disease detection\n",
        "\n",
        "Reduced diagnostic costs\n",
        "\n",
        "Improved patient outcomes\n",
        "\n",
        "Data-driven clinical decision support"
      ],
      "metadata": {
        "id": "9OhO-7GW5N_N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hD4nDW8Y5I1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}